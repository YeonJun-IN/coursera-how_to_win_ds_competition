{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from random import sample\n",
    "seed_list = list(range(10000))\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\data')\n",
    "\n",
    "TODAY = str(datetime.now().year)+str(datetime.now().month)+str(datetime.now().day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\data')\n",
    "\n",
    "train = pd.read_pickle('fixed2_train.pkl')\n",
    "test = pd.read_pickle(\"fixed2_test.pkl\")\n",
    "input_var = pd.read_csv('fixed2_input_var.csv')\n",
    "\n",
    "sub = pd.read_csv('submission_sample.csv')\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\data\\\\raw')\n",
    "bus = pd.read_csv('bus_bts.csv')\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\code\\\\experiment')\n",
    "experiment_db = pd.read_csv('experiment_DB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "added = []\n",
    "input_var = list(input_var.columns)\n",
    "target=['ride_18_20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "all['in_out_bus_route_id'] = all['bus_route_id'].astype('str') + all['in_out'].astype('str')\n",
    "\n",
    "temp = all.groupby('in_out_bus_route_id')['id'].count().to_dict()\n",
    "\n",
    "all['inout_bus_route_id_freq'] = all['in_out_bus_route_id'].map(temp)\n",
    "\n",
    "temp = all.groupby('in_out_bus_route_id')['ride_6_12'].agg(['mean','min','max','sum']).rename(\n",
    "columns = {\n",
    "    'mean' : 'inout_bus_route_id'+ '_' + 'ride_6_12' +'_'+'mean',\n",
    "    'min' : 'inout_bus_route_id'+ '_' + 'ride_6_12' +'_'+'min',\n",
    "    'max' : 'inout_bus_route_id'+ '_' + 'ride_6_12' +'_'+'max',\n",
    "    'sum' : 'inout_bus_route_id'+ '_' + 'ride_6_12' +'_'+'sum'\n",
    "})\n",
    "\n",
    "all = pd.merge(all,temp,how='left',on='in_out_bus_route_id')\n",
    "\n",
    "temp = all.groupby('in_out_bus_route_id')['takeoff_6_12'].agg(['mean','min','max','sum']).rename(\n",
    "columns = {\n",
    "    'mean' : 'inout_bus_route_id'+ '_' + 'takeoff_6_12' +'_'+'mean',\n",
    "    'min' : 'inout_bus_route_id'+ '_' + 'takeoff_6_12' +'_'+'min',\n",
    "    'max' : 'inout_bus_route_id'+ '_' + 'takeoff_6_12' +'_'+'max',\n",
    "    'sum' : 'inout_bus_route_id'+ '_' + 'takeoff_6_12' +'_'+'sum'\n",
    "})\n",
    "\n",
    "all = pd.merge(all,temp,how='left',on='in_out_bus_route_id')\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if 'inout_' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "all['diff_ride_takeoff'] = all['ride_6_12'] - all['takeoff_6_12']\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if 'diff_ride_takeoff' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "all['bus_route_id_station_code_concat'] = str(all['station_code']) + str(all['bus_route_id'])\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "# all['weekday_station_code_concat'] = str(all['station_code']) + str(all['weekday_var'])\n",
    "\n",
    "# train = all.loc[:(train.shape[0]-1),]\n",
    "# test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "# del all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "temp = all.groupby('station_name')['station_code'].nunique().reset_index().rename(columns={'station_code':'code_num_per_name'})\n",
    "\n",
    "all = pd.merge(all,temp,how='left',on='station_name')\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "# added += [a for a in train.columns if 'code_num_per_name' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "# all['station_travel'] = np.where(all['station_name'].str.contains('호텔'),'hotel',\n",
    "#         np.where(all['station_name'].str.contains('콘도'),'condo',\n",
    "#         np.where(all['station_name'].str.contains('공항'),'airport',\n",
    "#         np.where(all['station_name'].str.contains('터미널'),'terminal',\n",
    "#         np.where(all['station_name'].str.contains('초등학교'),'cho',\n",
    "#                 np.where(all['station_name'].str.contains('중학교'),'jung',\n",
    "#                 np.where(all['station_name'].str.contains('고등학교'),'ko',\n",
    "#                         np.where(all['station_name'].str.contains('대학'),'dae',\n",
    "#                                  np.where(all['station_name'].str.contains('환승'),'hwan',\n",
    "#                                           np.where(all['station_name'].str.contains('거리'),'street','etc'))))))))))\n",
    "\n",
    "\n",
    "# all = pd.concat([all,temp],axis=1)\n",
    "\n",
    "# train = all.loc[:(train.shape[0]-1),]\n",
    "# test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "# del all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "# all['6~7_diff_rt'] = all['ride_6_7'] - all['takeoff_6_7']\n",
    "# all['7~8_diff_rt'] = all['ride_7_8'] - all['takeoff_7_8']\n",
    "# all['8~9_diff_rt'] = all['ride_8_9'] - all['takeoff_8_9']\n",
    "# all['9~10_diff_rt'] = all['ride_9_10'] - all['takeoff_9_10']\n",
    "# all['10~11_diff_rt'] = all['ride_10_11'] - all['takeoff_10_11']\n",
    "# all['11~12_diff_rt'] = all['ride_11_12'] - all['takeoff_11_12']\n",
    "\n",
    "# train = all.loc[:(train.shape[0]-1),]\n",
    "# test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "# del all\n",
    "\n",
    "# added += [a for a in train.columns if 'diff_rt' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "# all['6~7_plus_rt'] = all['ride_6_7'] + all['takeoff_6_7']\n",
    "# all['7~8_plus_rt'] = all['ride_7_8'] + all['takeoff_7_8']\n",
    "# all['8~9_plus_rt'] = all['ride_8_9'] + all['takeoff_8_9']\n",
    "# all['9~10_plus_rt'] = all['ride_9_10'] + all['takeoff_9_10']\n",
    "# all['10~11_plus_rt'] = all['ride_10_11'] + all['takeoff_10_11']\n",
    "# all['11~12_plus_rt'] = all['ride_11_12'] + all['takeoff_11_12']\n",
    "\n",
    "# train = all.loc[:(train.shape[0]-1),]\n",
    "# test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "# del all\n",
    "\n",
    "# added += [a for a in train.columns if 'plus_rt' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "# all['all_6_12'] =  all['ride_6_12'] + all['takeoff_6_12']\n",
    "\n",
    "# all['all_6_12_ride*takeoff'] = all['ride_6_12']*all['takeoff_6_12']\n",
    "# # all['all_6_12_ride*/takeoff'] = (all['ride_6_12'])/(all['takeoff_6_12']+1)\n",
    "\n",
    "# train = all.loc[:(train.shape[0]-1),]\n",
    "# test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "# del all\n",
    "\n",
    "# added += [a for a in train.columns if 'all_6_12' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "# temp = bus.groupby('bus_route_id')['vhc_id'].nunique().to_dict()\n",
    "# all['vhc_id_num'] = all['bus_route_id'].map(temp).fillna(0)\n",
    "\n",
    "# temp = all.groupby('vhc_id_num')['id'].count().to_dict()\n",
    "\n",
    "# all['vhc_id_num_freq'] = all['vhc_id_num'].map(temp).fillna(0)\n",
    "\n",
    "# train = all.loc[:(train.shape[0]-1),]\n",
    "# test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "# del all\n",
    "\n",
    "# added += [a for a in train.columns if 'vhc_id_num' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haversine import haversine\n",
    "# all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "# dis = all[['station_code','latitude','longitude']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# start = datetime.now()\n",
    "# near = []\n",
    "# for aa,bb in zip(dis.latitude, dis.longitude):    \n",
    "#     dis_ar = np.array([haversine((aa,bb),(a,b)) for a,b in zip(dis.latitude, dis.longitude)])\n",
    "#     temp = np.where(dis_ar == 0,100.0,dis_ar)\n",
    "#     near += [dis.loc[np.argmin(temp),'station_code']]\n",
    "    \n",
    "# end = datetime.now()\n",
    "# print(end - start)\n",
    "\n",
    "# dis['near_station_code'] = near\n",
    "\n",
    "# temp = pd.merge(dis,\n",
    "#          train.groupby('station_code')['takeoff_6_12'].mean().reset_index().\\\n",
    "#          rename(columns={'station_code' : 'near_station_code'}),how='left',on='near_station_code')[['station_code','takeoff_6_12']].\\\n",
    "# rename(columns= {'takeoff_6_12':'takeoff_6_12_y'})\n",
    "\n",
    "# all = pd.merge(all,temp,how='left',on='station_code')\n",
    "\n",
    "# all['takeoff_6_12_y'] = all['takeoff_6_12_y'].fillna(0)\n",
    "\n",
    "# train = all.loc[:(train.shape[0]-1),]\n",
    "# test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "# del all\n",
    "\n",
    "# added += [a for a in train.columns if 'takeoff_6_12_y' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "code_which = all[['station_code','latitude','longitude']].drop_duplicates().reset_index(drop=True).rename(columns = {\n",
    "    'station_code' : 'geton_station_code',\n",
    "    'latitude' : 'geton_lat',\n",
    "    'longitude' : 'geton_long'\n",
    "})\n",
    "bus = pd.merge(bus,code_which, how='left',on='geton_station_code')\n",
    "\n",
    "code_which = all[['station_code','latitude','longitude']].drop_duplicates().reset_index(drop=True).rename(columns = {\n",
    "    'station_code' : 'getoff_station_code',\n",
    "    'latitude' : 'getoff_lat',\n",
    "    'longitude' : 'getoff_long'\n",
    "})\n",
    "bus = pd.merge(bus,code_which, how='left',on='getoff_station_code')\n",
    "\n",
    "all = pd.merge(all,pd.DataFrame({'station_code':bus[bus['geton_station_code'] == bus['getoff_station_code']].geton_station_code.unique(),\n",
    "             'same_on_off' : 1}),how='left',on='station_code') \n",
    "\n",
    "all['same_on_off'] = all['same_on_off'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "geton = []\n",
    "getoff = []\n",
    "for aa,bb,cc,dd in zip(bus['geton_lat'],bus['geton_long'],bus['getoff_lat'],bus['getoff_long']):\n",
    "    a = (aa,bb)\n",
    "    b = (cc,dd)\n",
    "    geton += [a]\n",
    "    getoff += [b]\n",
    "    \n",
    "from haversine import haversine\n",
    "\n",
    "dis = []\n",
    "for on,off in zip(geton,getoff):\n",
    "    dis += [haversine(on,off)]\n",
    "    \n",
    "bus['moving_dis'] = dis\n",
    "\n",
    "temp = bus.groupby('bus_route_id')['moving_dis'].mean().fillna(0).to_dict()\n",
    "\n",
    "all['moving_dis_per_bus'] = all['bus_route_id'].map(temp)\n",
    "\n",
    "temp = bus.groupby('geton_station_code')['moving_dis'].mean().fillna(0).to_dict()\n",
    "\n",
    "all['moving_dis_per_geton'] = all['station_code'].map(temp)\n",
    "\n",
    "temp = bus.groupby('getoff_station_code')['moving_dis'].mean().fillna(0).to_dict()\n",
    "\n",
    "all['moving_dis_per_getoff'] = all['station_code'].map(temp)\n",
    "\n",
    "all['moving_dis_per_bus'] =  all['moving_dis_per_bus'].fillna(all['moving_dis_per_bus'].median()) \n",
    "all['moving_dis_per_getoff'] = all['moving_dis_per_getoff'].fillna(all['moving_dis_per_getoff'].median()) \n",
    "all['moving_dis_per_geton'] = all['moving_dis_per_geton'].fillna(all['moving_dis_per_geton'].median()) \n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if 'moving_dis' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "bus['travel'] = bus['user_card_id'].map(bus['user_card_id'].value_counts()[bus['user_card_id'].value_counts()<10].to_dict())\n",
    "bus['travel'] = np.where(bus['travel'].isnull(),1,0)\n",
    "temp = bus.groupby('geton_station_code')['travel'].sum().to_dict()\n",
    "\n",
    "all['travel1'] = all['station_code'].map(temp).fillna(0)\n",
    "\n",
    "\n",
    "bus['travel'] = bus['user_card_id'].map(bus['user_card_id'].value_counts()[bus['user_card_id'].value_counts()<10].to_dict())\n",
    "bus['travel'] = np.where(bus['travel'].isnull(),1,0)\n",
    "temp = bus.groupby('getoff_station_code')['travel'].sum().to_dict()\n",
    "\n",
    "all['travel2'] = all['station_code'].map(temp).fillna(0)\n",
    "\n",
    "bus['travel'] = bus['user_card_id'].map(bus['user_card_id'].value_counts()[bus['user_card_id'].value_counts()<10].to_dict())\n",
    "bus['travel'] = np.where(bus['travel'].isnull(),0,1)\n",
    "temp = bus.groupby('geton_station_code')['travel'].sum().to_dict()\n",
    "\n",
    "all['travel3'] = all['station_code'].map(temp).fillna(0)\n",
    "\n",
    "bus['travel'] = bus['user_card_id'].map(bus['user_card_id'].value_counts()[bus['user_card_id'].value_counts()<10].to_dict())\n",
    "bus['travel'] = np.where(bus['travel'].isnull(),0,1)\n",
    "temp = bus.groupby('getoff_station_code')['travel'].sum().to_dict()\n",
    "\n",
    "all['travel4'] = all['station_code'].map(temp).fillna(0)\n",
    "\n",
    "# all['travel1'] = round(all['travel1']/(all['travel1'].max()),4)\n",
    "# all['travel2'] = round(all['travel2']/(all['travel2'].max()),4)\n",
    "# all['travel3'] = round(all['travel3']/(all['travel3'].max()),4)\n",
    "# all['travel4'] = round(all['travel4']/(all['travel4'].max()),4)\n",
    "\n",
    "\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all, bus\n",
    "\n",
    "added += ['travel1','travel2','travel3','travel4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "temp = np.power(all[['ride_6_7','ride_7_8','ride_8_9','ride_9_10','ride_10_11','ride_11_12','takeoff_6_7','takeoff_7_8','takeoff_8_9','takeoff_9_10','takeoff_10_11','takeoff_11_12']],2)\n",
    "temp.columns = [a+'_power' for a in tuple(temp.columns)]\n",
    "\n",
    "all = pd.concat([all,temp],axis=1)\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if 'power' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance \n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "jeju=(33.51411, 126.52969) # 제주 측정소 근처\n",
    "gosan=(33.29382, 126.16283) #고산 측정소 근처\n",
    "seongsan=(33.38677, 126.8802) #성산 측정소 근처\n",
    "po=(33.24616, 126.5653) #서귀포 측정소 근처\n",
    "\n",
    "t1 = [geopy.distance.vincenty( (i,j), jeju).km for i,j in list( zip( all['latitude'],all['longitude'] )) ]\n",
    "t2 = [geopy.distance.vincenty( (i,j), gosan).km for i,j in list( zip( all['latitude'],all['longitude'] )) ]\n",
    "t3 = [geopy.distance.vincenty( (i,j), seongsan).km for i,j in list( zip( all['latitude'],all['longitude'] )) ]\n",
    "t4 = [geopy.distance.vincenty( (i,j), po).km for i,j in list( zip( all['latitude'],all['longitude'] )) ]\n",
    "\n",
    "all['dis_jeju']=t1\n",
    "all['dis_gosan']=t2\n",
    "all['dis_seongsan']=t3\n",
    "all['dis_po']=t4\n",
    "\n",
    "all['dist_name'] = all[['dis_jeju','dis_gosan','dis_seongsan','dis_po']].apply(lambda x: np.argmin(x),axis=1).str.slice(4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "added += ['dis_jeju','dis_gosan','dis_seongsan','dis_po']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = input_var + added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\code\\\\experiment'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\data')\n",
    "all.loc[:(train.shape[0]-1),].to_pickle('fixed3_train.pkl')\n",
    "all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True).to_pickle('fixed3_test.pkl')\n",
    "\n",
    "pd.DataFrame(columns = input_var).to_csv('fixed3_input_var.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
